{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRDypMBlycV1s/raAISzWK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omeryanai/omeryanai/blob/main/Copy_of_BGU_DL_ass1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkvRp_GweSEt",
        "outputId": "117ff5f9-140f-4ba5-ef80-34ffc0f81552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.9.16\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pickle"
      ],
      "metadata": {
        "id": "-uwrJDpzoLbE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.\tinitialize_parameters(layer_dims)\n",
        "\n",
        "input: an array of the dimensions of each layer in the network (layer 0 is the size of the flattened input, layer L is the output softmax) <br>\n",
        "\n",
        "Example with input layer 784 neurons, output layer 10 neurons:  layer_dims = [784,30,20,10]<br>\n",
        "\n",
        "output: a dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL).\n",
        "\n",
        "Hint: Use the randn and zeros functions of numpy to initialize W and b, respectively\n"
      ],
      "metadata": {
        "id": "8vN7FUZblyYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(layer_dims,verbose=False):\n",
        "  parameters = {}\n",
        "  num_layers=layer_dims.shape[0]\n",
        "\n",
        "\n",
        "  for current_layer in range(1,num_layers):\n",
        "    prev_layer_size=layer_dims[current_layer-1]\n",
        "    curr_layer_size=layer_dims[current_layer]\n",
        "    w=np.random.randn(curr_layer_size,prev_layer_size)*0.01\n",
        "    B=np.zeros((curr_layer_size,1))*0.01\n",
        "    parameters['W'+str(current_layer)]=w\n",
        "    parameters['B'+str(current_layer)]=B\n",
        "    if verbose:\n",
        "      print('layer ',current_layer,' W ',w.shape,' B ',B.shape)\n",
        "\n",
        "  return parameters"
      ],
      "metadata": {
        "id": "haVSdiOHeUh0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.\tlinear_forward(A, W, b)\n",
        "\n",
        "Description: Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "input: \n",
        "A – the activations of the previous layer\n",
        "W – the weight matrix of the current layer (of shape [size of current layer, size of previous layer])\n",
        "B – the bias vector of the current layer (of shape [size of current layer, 1])\n",
        "\n",
        "Output:\n",
        "Z – the linear component of the activation function (i.e., the value before applying the non-linear function)\n",
        "linear_cache – a dictionary containing A, W, b (stored for making the backpropagation easier to compute)\n"
      ],
      "metadata": {
        "id": "cQSIddbgzraI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_forward(A_prev, W, b,verbose=False):\n",
        "  if verbose:\n",
        "    print('A_prev',A_prev.shape, 'W.T ',W.T.shape,' B ',b.shape)\n",
        " # Z=np.add(b,np.matmul(A,W.T))\n",
        "\n",
        "  Z = np.dot(W, A_prev) + b\n",
        "  if verbose:\n",
        "    print('W ',W.shape,'A_prev ',A_prev.shape, ' B ',b.shape,'Z',Z.shape)\n",
        "  linear_cache= {'A_prev':A_prev,'W':W,'B':b,'Z':Z}\n",
        "  return Z,linear_cache\n"
      ],
      "metadata": {
        "id": "OQEb-fegzvSY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c.\tsoftmax(Z)\n",
        "\n",
        "Input:\n",
        "Z – the linear component of the activation function\n",
        "\n",
        "Output:\n",
        "A – the activations of the layer\n",
        "activation_cache – returns Z, which will be useful for the backpropagation\n",
        "\n",
        "note:\n",
        "Softmax can be thought of as a sigmoid for multi-class problems. The formula for softmax for each node in the output layer is as follows:\n",
        "softmax = exp(zi) / Sum(exp(zi))"
      ],
      "metadata": {
        "id": "9ONvG5SvJZRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(Z):\n",
        "  A= (np.exp(Z)/np.exp(Z).sum())\n",
        "#  activation_cache={'Z':Z}\n",
        "  activation_cache={'A':A}\n",
        "  return A,activation_cache     # A is the softmax result  ,  activation cache with Z is  for backprop "
      ],
      "metadata": {
        "id": "dpOiNMMCJeSa"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d.\trelu(Z)\n",
        "Input:\n",
        "Z – the linear component of the activation function\n",
        "\n",
        "Output:\n",
        "A – the activations of the layer\n",
        "activation_cache – returns Z, which will be useful for the backpropagation\n"
      ],
      "metadata": {
        "id": "-0BtXIqAPEew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu(Z):\n",
        "  A=np.maximum(0,Z)\n",
        "# activation_cache={'Z':Z}\n",
        "  activation_cache={'A':A}\n",
        "  return A,activation_cache     # A is the softmax result  ,  activation cache with Z is  for backprop "
      ],
      "metadata": {
        "id": "xIprg4P_K1Nu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e.\tlinear_activation_forward(A_prev, W, B, activation)\n",
        "Description:\n",
        "Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "Input:\n",
        "A_prev – activations of the previous layer\n",
        "W – the weights matrix of the current layer\n",
        "B – the bias vector of the current layer\n",
        "Activation – the activation function to be used (a string, either “softmax” or “relu”)\n",
        "\n",
        "Output:\n",
        "A – the activations of the current layer\n",
        "cache – a joint dictionary containing both linear_cache and activation_cache\n"
      ],
      "metadata": {
        "id": "8UkMtUHXQf2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_forward(A_prev, W, B, activation,verbose=False):\n",
        "  linear_cache={}\n",
        "  linear_activation={}\n",
        "  Z,linear_cache=linear_forward(A_prev,W,B)\n",
        "  if activation=='softmax':\n",
        "    A,activation_cache=softmax(Z)\n",
        "  else:   # relu\n",
        "    A,activation_cache=relu(Z)\n",
        "  if verbose:\n",
        "    print('linear_activation_forward Z',A_prev.shape)\n",
        "    print('linear_activation_forward A',A.shape)\n",
        "    #print(linear_cache)\n",
        "    #print(activation_cache)\n",
        "  cache=linear_cache | activation_cache \n",
        "  return A,cache\n"
      ],
      "metadata": {
        "id": "SQ_L1jBmQh4B"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f.\tL_model_forward(X, parameters, use_batchnorm)\n",
        "Description:\n",
        "Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation\n",
        "\n",
        "Input:\n",
        "X – the data, numpy array of shape (input size, number of examples)\n",
        "parameters – the initialized W and b parameters of each layer\n",
        "use_batchnorm - a boolean flag used to determine whether to apply batchnorm after the activation (note that this option needs to be set to “false” in Section 3 and “true” in Section 4).\n",
        "\n",
        "Output:\n",
        "AL – the last post-activation value\n",
        "caches – a list of all the cache objects generated by the linear_forward function\n"
      ],
      "metadata": {
        "id": "_mLuDO3gWASh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def  L_model_forward(X, parameters, number_of_layers,use_batchnorm=False,verbose=False):\n",
        " \n",
        "    if not(use_batchnorm):\n",
        "        A=X  # for layer 0 \n",
        "        caches=[]\n",
        "        #caches.append(['Inputs'+str(0),A])\n",
        "\n",
        "        for l in range(1,number_of_layers-1):\n",
        "            A_prev=A\n",
        "            if verbose:\n",
        "              print('layer ',l,' A ',A.shape,A)\n",
        "              print(parameters['W'+str(l)].shape,parameters['B'+str(l)].shape)\n",
        "            A,c = linear_activation_forward(A_prev,parameters['W'+str(l)],parameters['B'+str(l)],'relu')\n",
        "            caches.append(['Layer'+str(l),c])\n",
        "\n",
        "        l=l+1   # for last layer  \n",
        "        AL,c= linear_activation_forward(A,parameters['W'+str(l)],parameters['B'+str(l)],'softmax')\n",
        "        caches.append(['Layer'+str(l),c])\n",
        "    return AL,caches"
      ],
      "metadata": {
        "id": "v89GWhhYWBn6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "g.\tcompute_cost(AL, Y)\n",
        "Description:\n",
        "Implement the cost function defined by equation. The requested cost function is categorical cross-entropy loss. The formula is as follows \n",
        "\n",
        "-1/m * sum( y*log(Ypred) )\n",
        "\n",
        "Input:\n",
        "AL – probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)\n",
        "Y – the labels vector (i.e. the ground truth)\n",
        "Output:\n",
        "cost – the cross-entropy cost\n"
      ],
      "metadata": {
        "id": "W4BpldtJDdvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(AL, Y,verbose=False):\n",
        "  m=Y.shape[1]\n",
        "  J=-(1/m)*np.multiply(Y,np.log(AL)).sum()\n",
        "  if verbose:\n",
        "    print('J is ',J)\n",
        "    print('AL ',AL.shape,' log ',np.log(AL).shape,'Y',Y.shape)\n",
        "    print('cost is',J,'cost shape',J.shape)\n",
        "  return J\n"
      ],
      "metadata": {
        "id": "KJCfI4iB-PXd"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.\tLinear_backward(dZ, cache)\n",
        "description:\n",
        "Implements the linear part of the backward propagation process for a single layer\n",
        "\n",
        "Input:\n",
        "dZ – the gradient of the cost with respect to the linear output of the current layer (layer l)\n",
        "cache – tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "Output:\n",
        "dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "db -- Gradient of the cost with respect to b (current layer l), same shape as b\n"
      ],
      "metadata": {
        "id": "kNWRHa8y-w8G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Linear_backward(dZ, cache,verbose=False):\n",
        "  A_prev=cache['A_prev']\n",
        "  W=cache['W']\n",
        "  B=cache['B']\n",
        "  \n",
        "  m=A_prev.shape[1]\n",
        "\n",
        "  dW=1/m*np.matmul(dZ,A_prev.T)\n",
        "  dA_prev=np.dot(W.T, dZ)   ##### not clear how to compute dA   #### this code from https://datascience-enthusiast.com/DL/Building-your-Deep-Neural-Network-Step-by-Step.html\n",
        "  dB=1/m*np.mean(dZ,axis=1,keepdims=True)\n",
        "  \n",
        "  if verbose:\n",
        "    print('dZ',dZ.shape,'A_prev',A_prev.shape,'W',W.shape,'B',B.shape,'dA_prev',dA_prev.shape,'dW',dW.shape,'dB',dB.shape)\n",
        "  return dA_prev, dW, dB"
      ],
      "metadata": {
        "id": "AZpeJnD_-v_p"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.\tlinear_activation_backward(dA, cache, activation)\n",
        "Description:\n",
        "Implements the backward propagation for the LINEAR->ACTIVATION layer. The function first computes dZ and then applies the linear_backward function.\n",
        "\n",
        "Some comments:\n",
        "The derivative of ReLU is 1 if x>0 , and 0 otherwise.\n",
        "\n",
        "The derivative of the softmax function is: p_i-y_i, where p_i is the softmax-adjusted probability of the class and y_i is the “ground truth” (i.e. 1 for the real class, 0 for all others) \n",
        "You should use the activations cache created earlier for the calculation of the activation derivative and the linear cache should be fed to the linear_backward function\n",
        "\n",
        "Input:\n",
        "dA – post activation gradient of the current layer\n",
        "cache – contains both the linear cache and the activations cache\n",
        "\n",
        "Output:\n",
        "dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "dW – Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "db – Gradient of the cost with respect to b (current layer l), same shape as b\n"
      ],
      "metadata": {
        "id": "tDXich6iakRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_activation_backward(dA, Y, cache, activation,verbose=False):\n",
        "  if activation=='relu':\n",
        "    dZ=relu_backward(dA,cache['Z'])\n",
        "    #print('relu derivative from dA to dZ ???')\n",
        "  if activation=='softmax':\n",
        "    dZ=softmax_backward(dA,Y, cache['A'])\n",
        "  \n",
        "  dA_prev, dW, dB = Linear_backward(dZ,cache,verbose=False)\n",
        "  if verbose:\n",
        "    print ('dZ ',dZ.shape )\n",
        "    print('dA_prev ',dA_prev)\n",
        "    print('cache ',cache)\n",
        "\n",
        "  return dA_prev, dW, dB"
      ],
      "metadata": {
        "id": "-eSh_CI3b3et"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "d.\tsoftmax_backward (dA, activation_cache)\n",
        "Description:\n",
        "Implements backward propagation for a softmax unit\n",
        "\n",
        "Input:\n",
        "dA – the post-activation gradient\n",
        "activation_cache – contains Z (stored during the forward propagation)\n",
        "\n",
        "Output:\n",
        "dZ – gradient of the cost with respect to Z\n"
      ],
      "metadata": {
        "id": "sJ3eNOgPcd_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax_backward(dA, Y ,activation_cache):\n",
        "\n",
        "  A= activation_cache\n",
        "  dZ=A-Y\n",
        "\n",
        "\n",
        " # y = da - a / ( -(2a+1 ))\n",
        "  return dZ"
      ],
      "metadata": {
        "id": "bybjKtRRbm1p"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "c.\trelu_backward (dA, activation_cache)\n",
        "Description:\n",
        "Implements backward propagation for a ReLU unit\n",
        "\n",
        "Input:\n",
        "dA – the post-activation gradient\n",
        "activation_cache – contains Z (stored during the forward propagation)\n",
        "\n",
        "Output:\n",
        "dZ – gradient of the cost with respect to Z\n"
      ],
      "metadata": {
        "id": "cGccPcz4cpDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_backward (dA, activation_cache):\n",
        "  A = activation_cache\n",
        "  dZ = np.array(A, copy=True)\n",
        "  dZ[A <= 0] = 0\n",
        "  #dZ=np.maximum(0,dA)\n",
        "  return dZ"
      ],
      "metadata": {
        "id": "1qaX-0HzcplR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "e.\tL_model_backward(AL, Y, caches)\n",
        "Description:\n",
        "Implement the backward propagation process for the entire network.\n",
        "\n",
        "Some comments:\n",
        "the backpropagation for the softmax function should be done only once as only the output layers uses it and the RELU should be done iteratively over all the remaining layers of the network. \n",
        "\n",
        "Input:\n",
        "AL - the probabilities vector, the output of the forward propagation (L_model_forward)\n",
        "Y - the true labels vector (the \"ground truth\" - true classifications)\n",
        "Caches - list of caches containing for each layer: a) the linear cache; b) the activation cache\n",
        "\n",
        "Output:\n",
        "Grads - a dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ...\n"
      ],
      "metadata": {
        "id": "KlnoXI6Ngc1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L_model_backward(AL, Y, caches,number_of_layers,verbose=False):\n",
        "      \n",
        "      grads={}\n",
        "\n",
        "      last_layer=number_of_layers-1  # for last layer\n",
        "      #print(AL.shape,Y.shape)\n",
        "      \n",
        "      dA = -np.divide(Y,AL)+(np.divide(1-Y,1-AL))\n",
        "\n",
        "      #dZ=AL-Y\n",
        "      l=last_layer\n",
        "      cache=caches[l-1][1]   # select cache of last layer\n",
        " ###############\n",
        "      #print('start backprop of layer' , l)\n",
        "      dA_prev, dW, dB=linear_activation_backward(dA, Y, cache,'softmax',verbose=False)\n",
        "      if verbose:\n",
        "              print('Received dA_prev',dA_prev.shape,'dW',dW.shape,'dB',dB.shape)\n",
        "\n",
        "      grads[\"dA\" + str(l)] = dA_prev \n",
        "      grads[\"dW\" + str(l)] = dW\n",
        "      grads[\"dB\" + str(l)] = dB\n",
        "\n",
        "      for l in range(last_layer-1,0,-1):\n",
        "            cache=caches[l-1][1]\n",
        "\n",
        "            if verbose:\n",
        "              print('layer ',l,' cache ',cache)\n",
        "              print('sending dA to lin_act_bck ',dA.shape)\n",
        "              print('dA',dA)\n",
        "              print('start backprop of layer' , l)\n",
        "            dA_prev, dW, dB=linear_activation_backward(dA, Y, cache,'relu',verbose=False)\n",
        "\n",
        "            grads[\"dA\" + str(l)] = dA_prev \n",
        "            grads[\"dW\" + str(l)] = dW\n",
        "            grads[\"dB\" + str(l)] = dB\n",
        "            if verbose:\n",
        "              print('dA',dA_prev.shape,'dW',dW.shape,'dB',dB.shape)\n",
        "         \n",
        "      return grads"
      ],
      "metadata": {
        "id": "iGrFMLMsgdBc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "f.\tUpdate_parameters(parameters, grads, learning_rate)\n",
        "Description:\n",
        "Updates parameters using gradient descent\n",
        "\n",
        "Input:\n",
        "parameters – a python dictionary containing the DNN architecture’s parameters\n",
        "grads – a python dictionary containing the gradients (generated by L_model_backward)\n",
        "learning_rate – the learning rate used to update the parameters (the “alpha”)\n",
        "\n",
        "Output:\n",
        "parameters – the updated values of the parameters object provided as input\n"
      ],
      "metadata": {
        "id": "GQ1z0PVgg4Ey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, grads, learning_rate,number_of_layers,verbose=False):\n",
        "   for l in range(number_of_layers-1,0,-1):    # there are 4 layers, so last layer is 3 \n",
        "        if verbose:\n",
        "          print(parameters)\n",
        "        W=parameters['W'+str(l)]\n",
        "        B=parameters['B'+str(l)]\n",
        "        dW=grads['dW'+str(l)]\n",
        "        dB=grads['dB'+str(l)]\n",
        "        if verbose:\n",
        "              print('layer ',l)\n",
        "              print('w',parameters['W'+str(l)].shape)\n",
        "              print('B',parameters['B'+str(l)].shape)\n",
        "              print('dW',dW.shape)\n",
        "              print('dB',dB.shape)\n",
        "              \n",
        "        new_W=W-learning_rate*dW\n",
        "        parameters['W'+str(l)]=new_W\n",
        "\n",
        "        new_B=B-learning_rate*dB\n",
        "        parameters['B'+str(l)]=new_B\n",
        "\n",
        "   return parameters\n"
      ],
      "metadata": {
        "id": "fXiv-EYxg4Nv"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "a.\tL_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size)\n",
        "Description:\n",
        "Implements a L-layer neural network. All layers but the last should have the ReLU activation function, and the final layer will apply the softmax activation function. The size of the output layer should be equal to the number of labels in the data. Please select a batch size that enables your code to run well (i.e. no memory overflows while still running relatively fast).\n",
        "\n",
        "Hint: the function should use the earlier functions in the following order: initialize -> L_model_forward -> compute_cost -> L_model_backward -> update parameters\n",
        "\n",
        "Input:\n",
        "X – the input data, a numpy array of shape (height*width , number_of_examples) \n",
        "Comment: since the input is in grayscale we only have height and width, otherwise it would have been height*width*3\n",
        "Y – the “real” labels of the data, a vector of shape (num_of_classes, number of examples)\n",
        "Layer_dims – a list containing the dimensions of each layer, including the input\n",
        "batch_size – the number of examples in a single training batch.\n",
        "\n",
        "Output:\n",
        "parameters – the parameters learnt by the system during the training (the same parameters that were updated in the update_parameters function).\n",
        "costs – the values of the cost function (calculated by the compute_cost function). One value is to be saved after each 100 training iterations (e.g. 3000 iterations -> 30 values).\n"
      ],
      "metadata": {
        "id": "T_vhFhX8gvtB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size,verbose=False):\n",
        "    # initialize network\n",
        "    num_layers=layers_dims.shape[0]\n",
        "    parameters=initialize_parameters(layers_dims,verbose=False)\n",
        "    #print(model)\n",
        "    # break X,Y into batches\n",
        "    for itter in range(num_iterations):\n",
        "      m=X.shape[1]\n",
        "      for idx in range(0,m,batch_size):      \n",
        "          X_batch = X[:,idx:min(m,idx+batch_size)]  \n",
        "          Y_batch = Y[:,idx:min(m,idx+batch_size)] \n",
        "          AL,caches=L_model_forward(X_batch,parameters,num_layers,False,verbose=False)\n",
        "\n",
        "          #print(caches)\n",
        "\n",
        "          J=compute_cost(AL,Y_batch,verbose=False)\n",
        "\n",
        "          \n",
        "\n",
        "          if verbose:\n",
        "                  print('itteration ',itter, 'index ',idx,'cost ',J)\n",
        "                  print('len caches',len(caches))\n",
        "                  for c in caches:\n",
        "                    print('chache ------------------')\n",
        "                    print(c)\n",
        "                    #print(c[0],c[1]['A'])         \n",
        "          grads=L_model_backward(AL, Y_batch, caches,num_layers,verbose=False)\n",
        "          \n",
        "          #print(grads)\n",
        "\n",
        "          parameters=update_parameters(parameters, grads, learning_rate,num_layers,verbose=False)\n",
        "          if verbose:\n",
        "            print('running batch ',idx,' to ',idx+batch_size,'   Activation results  of last layer:')\n",
        "            print('cost is ',J)\n",
        "            \n",
        "      print('itteration ',itter, 'index ',idx,'cost ',J)\n",
        "      \n",
        "      if verbose:\n",
        "        for c in caches:\n",
        "          print('Activations ------------------')\n",
        "          print(c[0],c[1]['A'])\n",
        "            \n",
        "    return caches,parameters\n",
        "    "
      ],
      "metadata": {
        "id": "IrCFrh-6gz7x"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_one_hot(y_in):     ### input is (m,)\n",
        "\n",
        "  m=y_in.shape[0]\n",
        "  y_t=y_in.reshape(m,1)\n",
        "  classes = y_t.max()+1\n",
        "  train_y = np.zeros((m, classes))\n",
        "#print(train_y.shape)\n",
        "#replacing 0 with a 1 at() the index of the original array\n",
        "  rows=np.arange(m)\n",
        "  train_y[rows,y_t] = 1\n",
        "  return train_y.T      ### output  (classes,m)"
      ],
      "metadata": {
        "id": "cQUKqaNotWez"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "b.\tPredict(X, Y, parameters)\n",
        "Description:\n",
        "The function receives an input data and the true labels and calculates the accuracy of the trained neural network on the data.\n",
        "\n",
        "Input:\n",
        "X – the input data, a numpy array of shape (height*width, number_of_examples)\n",
        "Y – the “real” labels of the data, a vector of shape (num_of_classes, number of examples)\n",
        "Parameters – a python dictionary containing the DNN architecture’s parameters\n",
        "\n",
        "Output:\n",
        "accuracy – the accuracy measure of the neural net on the provided data (i.e. the percentage of the samples for which the correct label receives the hughest confidence score). Use the softmax function to normalize the output values.\n"
      ],
      "metadata": {
        "id": "riN5ffqDq65n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Predict(X, Y, parameters, number_of_layers,verbose=False):\n",
        "        AL=X\n",
        "        for l in range(1,number_of_layers-1):\n",
        "            W=parameters['W'+str(l)]\n",
        "            B=parameters['B'+str(l)]\n",
        "            if verbose:\n",
        "              print('layer ',l,' of ',number_of_layers)\n",
        "              print(parameters['W'+str(l)].shape,parameters['B'+str(l)].shape)\n",
        "            A=AL\n",
        "            AL,c = linear_activation_forward(A,W,B,'relu')\n",
        "            #caches.append(['Layer'+str(l),c])\n",
        "\n",
        "        l=l+1   # for last layer  \n",
        "        W=parameters['W'+str(l)]\n",
        "        B=parameters['B'+str(l)]\n",
        "        Y_pred,c= linear_activation_forward(AL,W,B,'softmax')\n",
        "\n",
        "        a=np.argmax(Y_predict,axis=0)\n",
        "        b=np.argmax(Y,axis=0)\n",
        "        if verbose:\n",
        "          print(b.shape)\n",
        "          print(a,b)\n",
        "        accuracy = np.sum(Y_pred == Y)/b.shape[0]\n",
        "\n",
        "        #caches.append(['Layer'+str(l),c])\n",
        "        return accuracy,Y_pred"
      ],
      "metadata": {
        "id": "L4iGwczFq7P7"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##  testing training cycle \n",
        "\n",
        "l_dims=np.array([3,4,7,2])\n",
        "number_of_layers = len(l_dims)\n",
        "#print('num layers',number_of_layers)\n",
        "print('neurons per layer',l_dims)\n",
        "\n",
        "#\n",
        "X=np.random.rand(3,20)\n",
        "print('X - input ',X.shape)\n",
        "Y=np.array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 ,1,1, 1, 1,1, 1, 1],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]])\n",
        "print('Y - output ',Y.shape)    # Y dimension should be (m = number of examples,n = output layer neurons)\n",
        "\n",
        "#print(Y)\n",
        "print(\"------------------------\")\n",
        "cache,parameters = L_layer_model(X,Y,l_dims,0.01,5,5,verbose=False)\n",
        "\n",
        "\n",
        "with open('parameters.pickle', 'wb') as handle:\n",
        "    pickle.dump(parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "#print(cache)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxkNfbbXxM3f",
        "outputId": "bf3a80f2-8072-41bf-bcef-dd2aba3ffce2"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neurons per layer [3 4 7 2]\n",
            "X - input  (3, 20)\n",
            "Y - output  (2, 20)\n",
            "------------------------\n",
            "itteration  0 index  0 cost  2.3025847742290195\n",
            "itteration  0 index  5 cost  2.301585178546798\n",
            "itteration  0 index  10 cost  2.300587003873983\n",
            "itteration  0 index  15 cost  2.299589648354293\n",
            "itteration  1 index  0 cost  2.2985939827496438\n",
            "itteration  1 index  5 cost  2.2975991704774614\n",
            "itteration  1 index  10 cost  2.296605801746204\n",
            "itteration  1 index  15 cost  2.29561323384253\n",
            "itteration  2 index  0 cost  2.2946223533541055\n",
            "itteration  2 index  5 cost  2.293632315311992\n",
            "itteration  2 index  10 cost  2.292643740729141\n",
            "itteration  2 index  15 cost  2.2916559499565268\n",
            "itteration  3 index  0 cost  2.290669844297424\n",
            "itteration  3 index  5 cost  2.289684570820643\n",
            "itteration  3 index  10 cost  2.2887007781224376\n",
            "itteration  3 index  15 cost  2.2877177546872285\n",
            "itteration  4 index  0 cost  2.286736413283035\n",
            "itteration  4 index  5 cost  2.28575589460981\n",
            "itteration  4 index  10 cost  2.2847768718943438\n",
            "itteration  4 index  15 cost  2.2837986054825037\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Running Prediction\n",
        "\n",
        "with open('parameters.pickle', 'rb') as handle:\n",
        "    parameters = pickle.load(handle)\n",
        "\n",
        "l_dims=np.array([3,4,7,2])\n",
        "number_of_layers = len(l_dims)\n",
        "#print('num layers',number_of_layers)\n",
        "print('neurons per layer',l_dims)\n",
        "\n",
        "#\n",
        "X=np.random.rand(3,20)\n",
        "print('X - input ',X.shape)\n",
        "Y=np.array([[0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1 ,1,1, 1, 1,1, 1, 1],[1,1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]])\n",
        "print('Y - output ',Y.shape)    # Y dimension should be (m = number of examples,n = output layer neurons)\n",
        "\n",
        "accuracy,Y_predict = Predict(X, Y, parameters, number_of_layers,verbose=True)\n",
        "\n",
        "print('Accuracy is ',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXgupIr34XUI",
        "outputId": "a92ca195-851a-4794-ee6d-5795f471bab6"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "neurons per layer [3 4 7 2]\n",
            "X - input  (3, 20)\n",
            "Y - output  (2, 20)\n",
            "layer  1  of  4\n",
            "(4, 3) (4, 1)\n",
            "layer  2  of  4\n",
            "(7, 4) (7, 1)\n",
            "(20,)\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "Accuracy is  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eE8_GyNdQ33K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.\tUse the code you wrote to classify the MNIST dataset and present a summary report\n",
        "a.\tLoad the dataset using the Keras code. Note that there is a predefined division between the train and test set. Use 20% of the training set as a validation set (samples need to be randomly chosen).\n",
        "b.\tRun your network using the following configuration:\n",
        "•\t4 layers (aside from the input layer), with the following sizes: 20,7,5,10\n",
        "•\tDo not activate the batchnorm option at this point\n",
        "•\tThe input at each iteration needs to be “flattened” to a matrix of [m,784], where m is the number of samples\n",
        "•\tUse a learning rate of 0.009\n",
        "•\tTrain the network until there is no improvement on the validation set (or the improvement is very small) for 100 training steps (this is the stopping criterion). Please include in the report the number of iterations and epochs needed to train your network. Also, specify the batch size.\n",
        "c.\tPlease include the following details in your report:\n",
        "•\tThe final accuracy values for the train, validation and test sets.\n",
        "•\tThe cost value for each 100 training steps. Please make sure that the index of the training step will also be included in the report. Print the values from the L_layer_model\n",
        "d.\tAll the information requested above will be included in a .docx file uploaded with the code.\n"
      ],
      "metadata": {
        "id": "q2NR54YxQ4hS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j5gS3iIdFoF2"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##  MNIST training cycle \n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "verbose=False\n",
        "(x, y), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "X_train, X_valid, y_train, y_valid = train_test_split( x, y, test_size=0.2, random_state=42)\n",
        "if verbose:\n",
        "  print(X_train.shape)\n",
        "  print(X_valid.shape)\n",
        "  print(X_test.shape)\n",
        "  print(y_train.shape)\n",
        "  print(y_valid.shape)\n",
        "  print(y_test.shape)\n",
        "\n",
        "l_dims=np.array([784,20,7,5,10])\n",
        "number_of_layers = len(l_dims)\n",
        "if verbose:\n",
        "  #print('num layers',number_of_layers)\n",
        "  print('neurons per layer',l_dims)\n",
        "\n",
        "# \n",
        "\n",
        "train_x = X_train.reshape((-1, 784)).T\n",
        "\n",
        "train_y=make_one_hot(y_train)\n",
        "\n",
        "if verbose:\n",
        "  print('X - input ',X.shape)\n",
        "  print('Y - output ',Y.shape)    \n",
        "\n",
        "#print(Y)\n",
        "print(\"-----start train -------------------\")\n",
        "cache,parameters = L_layer_model(train_x,train_y,l_dims,0.1,10,50,verbose=False)\n",
        "\n",
        "\n",
        "with open('parameters.pickle', 'wb') as handle:\n",
        "    pickle.dump(parameters, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "#print(cache)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA28nICdFoIW",
        "outputId": "2ca9aeec-4ea5-41bd-8de0-2045741761e7"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----start train -------------------\n",
            "itteration  0 index  47950 cost  62.1460809842219\n",
            "itteration  1 index  47950 cost  62.146080984221925\n",
            "itteration  2 index  47950 cost  62.14608098422192\n",
            "itteration  3 index  47950 cost  62.14608098422192\n",
            "itteration  4 index  47950 cost  62.1460809842219\n",
            "itteration  5 index  47950 cost  62.1460809842219\n",
            "itteration  6 index  47950 cost  62.14608098422192\n",
            "itteration  7 index  47950 cost  62.14608098422191\n",
            "itteration  8 index  47950 cost  62.146080984221925\n",
            "itteration  9 index  47950 cost  62.146080984221925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  Running MNIST Prediction\n",
        "\n",
        "with open('parameters.pickle', 'rb') as handle:\n",
        "    parameters = pickle.load(handle)\n",
        "\n",
        "#\n",
        "X=X_valid.reshape((-1, 784)).T\n",
        "\n",
        "Y=make_one_hot(y_valid)\n",
        "\n",
        "accuracy,Y_predict = Predict(X, Y, parameters, 5 ,verbose=True)\n",
        "\n",
        "print('Accuracy is ',accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlmrmmtAwULy",
        "outputId": "adf542c4-6498-48fd-8a2b-013d4e081ca8"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer  1  of  5\n",
            "(20, 784) (20, 1)\n",
            "layer  2  of  5\n",
            "(7, 20) (7, 1)\n",
            "layer  3  of  5\n",
            "(5, 7) (5, 1)\n",
            "(12000,)\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 ... 0 0 0]\n",
            "Accuracy is  0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6PYelJW9FoOf"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IpQD3ZMjhKpI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6c558c4-5031-4628-a717-8968e8245222"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.02549902, 0.02549901, 0.025499  , 0.02549901, 0.02549899,\n",
              "        0.02549901, 0.02549898, 0.025499  , 0.02549898, 0.025499  ,\n",
              "        0.02549898, 0.02549899, 0.02549899, 0.02549897, 0.02549897,\n",
              "        0.025499  , 0.02549902, 0.02549899, 0.025499  , 0.02549898],\n",
              "       [0.02450101, 0.02450101, 0.02450102, 0.02450102, 0.024501  ,\n",
              "        0.02450099, 0.02450101, 0.02450102, 0.02450101, 0.02450102,\n",
              "        0.024501  , 0.024501  , 0.02450099, 0.024501  , 0.024501  ,\n",
              "        0.02450101, 0.02450101, 0.024501  , 0.02450101, 0.024501  ]])"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SWYlLi6VhKrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PyuCSm1_hKys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_cAdvXEChK1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Q0C6tYLhK4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H6c3pUwjhK9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####   Object Oriented Implementation of the Network\n",
        "\n",
        "class Dense_Layer:\n",
        "  def __init__(self,n_inputs,n_neurons):\n",
        "    self.weights = np.random.randn(n_inputs,n_neurons)\n",
        "    self.biases = np.zeros(n_neurons)\n",
        "  def f_forward(self,inputs):\n",
        "    self.outputs=np.dot(inputs,self.weights)+self.biases\n",
        "\n",
        "X_train = [[1,2,3,4],[5,6,7,8],[9,9,9,9]]\n",
        "Layer1 = Dense_Layer(4,5)\n",
        "Layer2 = Dense_Layer(5,2) \n",
        "\n",
        "Layer1.f_forward(X_train)\n",
        "Layer2.f_forward(Layer1.outputs)\n",
        "\n",
        "print(Layer2.outputs)\n"
      ],
      "metadata": {
        "id": "M5oF5NQzhKm2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}